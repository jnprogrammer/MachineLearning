{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJtu92ETVqHV2mQnp/OgT/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jnprogrammer/MachineLearning/blob/master/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_DLp2rv3tY9",
        "colab_type": "code",
        "outputId": "c0856310-0861-41e7-9388-f7b937953f19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Install TensorFlow\n",
        "# !pip install -q tensorflow-gpu==2.0.0-rc0\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x #colab only\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
            "You set: `2.x #colab only`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n",
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJqShbt14eQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# standard imports \n",
        "from tensorflow.keras.layers import Input, Dense, LeakyReLU, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sys, os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7nm12r-4xKV",
        "colab_type": "code",
        "outputId": "f5b185c7-744d-49cb-b952-3157b7dd262b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "#Load in the data \n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "#map inputs to (-1, +1) for better training\n",
        "x_train, x_test = x_train / 255.0 * 2 - 1, x_test / 255.0 * 2 - 1\n",
        "print(\"x_train.shape:\", x_train.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train.shape: (60000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHB0Gtnm5XcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Flatten the data\n",
        "\n",
        "N, H, W = x_train.shape\n",
        "D = H * W\n",
        "x_train = x_train.reshape(-1, D)\n",
        "x_test = x_test.reshape(-1, D)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tixOg9xzWMev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dimensionality of the latent space\n",
        "latent_dim = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuPcTx-cWRiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the generator model\n",
        "def build_generator(latent_dim):\n",
        "  i = Input(shape=(latent_dim,))\n",
        "  x = Dense(256, activation=LeakyReLU(alpha=0.2))(i)\n",
        "  x = BatchNormalization(momentum=0.8)(x)\n",
        "  x = Dense(512, activation=LeakyReLU(alpha=0.2))(x)\n",
        "  x = BatchNormalization(momentum=0.8)(x)\n",
        "  x = Dense(1024, activation=LeakyReLU(alpha=0.2))(x)\n",
        "  x = BatchNormalization(momentum=0.8)(x)\n",
        "  x = Dense(D, activation='tanh')(x)\n",
        "\n",
        "  model = Model(i, x)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZc4UGABXuYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the discriminator model\n",
        "def build_discriminator(img_size):\n",
        "  i = Input(shape=(img_size,))\n",
        "  x = Dense(512, activation=LeakyReLU(alpha=0.2))(i)\n",
        "  x = Dense(256, activation=LeakyReLU(alpha=0.2))(x)\n",
        "  x = Dense(1, activation='sigmoid')(x)\n",
        "  model = Model(i, x)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt4rqAj4ZPAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile both models in preparation for training\n",
        "\n",
        "# build and compile the discriminator\n",
        "discriminator = build_discriminator(D)\n",
        "discriminator.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=Adam(0.0002, 0.5),\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "#build and compile the combined model\n",
        "generator = build_generator(latent_dim)\n",
        "\n",
        "#Create an input to represent noise sample from latent space\n",
        "z = Input(shape=(latent_dim,))\n",
        "\n",
        "# pass noise through generator to get an impage\n",
        "img = generator(z)\n",
        "\n",
        "# Make sure only the generator is trained\n",
        "discriminator.traninable = False\n",
        "\n",
        "# The true output is fake, but we label them real!\n",
        "fake_pred = discriminator(img)\n",
        "\n",
        "# Create the combined model object\n",
        "combined_model = Model(z, fake_pred)\n",
        "\n",
        "#Compiled the combined model\n",
        "combined_model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002,0.))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJPG4eUffS0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the GAN\n",
        "\n",
        "# Config\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 30000\n",
        "sample_period = 200 # every 'sample_period' steps generate and save some data\n",
        "\n",
        "# Create batch labels to use when calling train_on_batch\n",
        "ones = np.ones(batch_size)\n",
        "zeros = np.zeros(batch_size)\n",
        "\n",
        "# Store the losses\n",
        "d_losses = []\n",
        "g_losses = []\n",
        "\n",
        "# Create a folder to store generated images\n",
        "if not os.path.exists('gan_images'):\n",
        "  os.makedirs('gan_images')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T7IIC9ZgOs8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to generate a grid of random samples from the generator\n",
        "# and save them a file\n",
        "def sample_images(epoch):\n",
        "  rows, cols = 5, 5\n",
        "  noise = np.random.randn(rows * cols, latent_dim)\n",
        "  imgs = generator.predict(noise)\n",
        "\n",
        "  # Rescale images 0 - 1\n",
        "  imgs = 0.5 * imgs + 0.5\n",
        "\n",
        "  fig, axs = plt.subplots(rows, cols)\n",
        "  idx = 0\n",
        "  for i in range(rows):\n",
        "    for j in range(cols):\n",
        "      axs[i, j].imshow(imgs[idx].reshape(H, W), cmap='gray')\n",
        "      axs[i, j].axis('off')\n",
        "      idx += 1\n",
        "  fig.savefig('gan_images/%d.png' % epoch)\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBkGQiV3insr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5895bb40-7706-433a-c5e5-0d00115dbe6d"
      },
      "source": [
        "# Main training loop\n",
        "for epoch in range(epochs):\n",
        "  # Train discriminator\n",
        "\n",
        "  # Select a random batch of images\n",
        "  idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
        "  real_imgs = x_train[idx]\n",
        "\n",
        "  #generate fake images\n",
        "  noise = np.random.randn(batch_size, latent_dim)\n",
        "  fake_imgs = generator.predict(noise)\n",
        "\n",
        "  #train the discriminator\n",
        "  #both loss and accuracy are returned\n",
        "  d_loss_real, d_acc_real = discriminator.train_on_batch(real_imgs, ones)\n",
        "  d_loss_fake, d_acc_fake = discriminator.train_on_batch(fake_imgs, zeros)\n",
        "  d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
        "  d_acc = 0.5 * (d_acc_real + d_acc_fake)\n",
        "\n",
        "  #Train generator\n",
        "  noise = np.random.randn(batch_size, latent_dim)\n",
        "  g_loss = combined_model.train_on_batch(noise, ones)\n",
        "\n",
        "  #save the losses\n",
        "  d_losses.append(d_loss)\n",
        "  g_losses.append(g_loss)\n",
        "\n",
        "  if epoch % 100 ==0:\n",
        "    print(f\"epoch: {epoch+1}/{epochs}, d_loss: {d_loss:.2f}, d_acc: {d_acc:.2f}, g_loss: {g_loss:.2f}\")\n",
        "  \n",
        "  if epoch % sample_period == 0:\n",
        "    sample_images(epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1/30000, d_loss: 0.36, d_acc: 0.80, g_loss: 0.64\n",
            "epoch: 101/30000, d_loss: 0.31, d_acc: 0.86, g_loss: 1.08\n",
            "epoch: 201/30000, d_loss: 0.27, d_acc: 0.88, g_loss: 1.01\n",
            "epoch: 301/30000, d_loss: 0.37, d_acc: 0.83, g_loss: 1.28\n",
            "epoch: 401/30000, d_loss: 0.61, d_acc: 0.72, g_loss: 1.03\n",
            "epoch: 501/30000, d_loss: 0.59, d_acc: 0.70, g_loss: 1.49\n",
            "epoch: 601/30000, d_loss: 0.58, d_acc: 0.75, g_loss: 1.42\n",
            "epoch: 701/30000, d_loss: 0.45, d_acc: 0.77, g_loss: 1.61\n",
            "epoch: 801/30000, d_loss: 0.45, d_acc: 0.70, g_loss: 1.51\n",
            "epoch: 901/30000, d_loss: 0.38, d_acc: 0.83, g_loss: 1.60\n",
            "epoch: 1001/30000, d_loss: 0.47, d_acc: 0.67, g_loss: 1.76\n",
            "epoch: 1101/30000, d_loss: 0.81, d_acc: 0.66, g_loss: 1.44\n",
            "epoch: 1201/30000, d_loss: 0.48, d_acc: 0.73, g_loss: 1.62\n",
            "epoch: 1301/30000, d_loss: 0.47, d_acc: 0.61, g_loss: 1.58\n",
            "epoch: 1401/30000, d_loss: 0.51, d_acc: 0.52, g_loss: 1.78\n",
            "epoch: 1501/30000, d_loss: 0.51, d_acc: 0.73, g_loss: 1.71\n",
            "epoch: 1601/30000, d_loss: 0.56, d_acc: 0.86, g_loss: 1.72\n",
            "epoch: 1701/30000, d_loss: 0.51, d_acc: 0.56, g_loss: 1.72\n",
            "epoch: 1801/30000, d_loss: 0.49, d_acc: 0.62, g_loss: 2.01\n",
            "epoch: 1901/30000, d_loss: 0.47, d_acc: 0.62, g_loss: 2.19\n",
            "epoch: 2001/30000, d_loss: 0.55, d_acc: 0.67, g_loss: 1.92\n",
            "epoch: 2101/30000, d_loss: 0.64, d_acc: 0.73, g_loss: 1.81\n",
            "epoch: 2201/30000, d_loss: 0.54, d_acc: 0.66, g_loss: 2.11\n",
            "epoch: 2301/30000, d_loss: 0.58, d_acc: 0.70, g_loss: 1.84\n",
            "epoch: 2401/30000, d_loss: 0.59, d_acc: 0.69, g_loss: 1.90\n",
            "epoch: 2501/30000, d_loss: 0.54, d_acc: 0.72, g_loss: 1.67\n",
            "epoch: 2601/30000, d_loss: 0.69, d_acc: 0.62, g_loss: 1.96\n",
            "epoch: 2701/30000, d_loss: 0.59, d_acc: 0.75, g_loss: 1.92\n",
            "epoch: 2801/30000, d_loss: 0.51, d_acc: 0.70, g_loss: 1.75\n",
            "epoch: 2901/30000, d_loss: 0.52, d_acc: 0.64, g_loss: 2.29\n",
            "epoch: 3001/30000, d_loss: 0.68, d_acc: 0.52, g_loss: 2.02\n",
            "epoch: 3101/30000, d_loss: 0.62, d_acc: 0.72, g_loss: 1.77\n",
            "epoch: 3201/30000, d_loss: 0.50, d_acc: 0.59, g_loss: 1.86\n",
            "epoch: 3301/30000, d_loss: 0.53, d_acc: 0.72, g_loss: 1.69\n",
            "epoch: 3401/30000, d_loss: 0.66, d_acc: 0.64, g_loss: 2.15\n",
            "epoch: 3501/30000, d_loss: 0.61, d_acc: 0.50, g_loss: 2.04\n",
            "epoch: 3601/30000, d_loss: 0.79, d_acc: 0.58, g_loss: 2.05\n",
            "epoch: 3701/30000, d_loss: 0.72, d_acc: 0.61, g_loss: 1.73\n",
            "epoch: 3801/30000, d_loss: 0.63, d_acc: 0.61, g_loss: 1.83\n",
            "epoch: 3901/30000, d_loss: 0.69, d_acc: 0.50, g_loss: 1.78\n",
            "epoch: 4001/30000, d_loss: 0.71, d_acc: 0.66, g_loss: 3.97\n",
            "epoch: 4101/30000, d_loss: 0.55, d_acc: 0.62, g_loss: 1.95\n",
            "epoch: 4201/30000, d_loss: 0.53, d_acc: 0.50, g_loss: 2.05\n",
            "epoch: 4301/30000, d_loss: 0.58, d_acc: 0.50, g_loss: 2.45\n",
            "epoch: 4401/30000, d_loss: 0.57, d_acc: 0.50, g_loss: 2.05\n",
            "epoch: 4501/30000, d_loss: 0.60, d_acc: 0.50, g_loss: 2.04\n",
            "epoch: 4601/30000, d_loss: 0.57, d_acc: 0.56, g_loss: 1.88\n",
            "epoch: 4701/30000, d_loss: 0.57, d_acc: 0.50, g_loss: 2.34\n",
            "epoch: 4801/30000, d_loss: 0.40, d_acc: 0.77, g_loss: 1.95\n",
            "epoch: 4901/30000, d_loss: 0.51, d_acc: 0.66, g_loss: 2.06\n",
            "epoch: 5001/30000, d_loss: 0.45, d_acc: 0.80, g_loss: 2.63\n",
            "epoch: 5101/30000, d_loss: 0.55, d_acc: 0.50, g_loss: 2.29\n",
            "epoch: 5201/30000, d_loss: 0.57, d_acc: 0.61, g_loss: 2.82\n",
            "epoch: 5301/30000, d_loss: 0.48, d_acc: 0.50, g_loss: 2.68\n",
            "epoch: 5401/30000, d_loss: 0.59, d_acc: 0.50, g_loss: 2.89\n",
            "epoch: 5501/30000, d_loss: 0.39, d_acc: 0.73, g_loss: 2.30\n",
            "epoch: 5601/30000, d_loss: 0.45, d_acc: 0.78, g_loss: 2.68\n",
            "epoch: 5701/30000, d_loss: 0.49, d_acc: 0.61, g_loss: 2.40\n",
            "epoch: 5801/30000, d_loss: 0.60, d_acc: 0.50, g_loss: 2.48\n",
            "epoch: 5901/30000, d_loss: 0.46, d_acc: 0.73, g_loss: 2.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATSW8gdzonNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(g_losses, label='g_losses')\n",
        "plt.plot(d_losses, label='d_losses')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pbw0U0QMp1Os",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls gan_images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8Of46mlqH5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skimge.io import imread\n",
        "a = imread('gan_images/0.png')\n",
        "plt.imshow(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HV_D0kxqR8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = imread('gan_images/1000.png')\n",
        "plt.imshow(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOzF6GtRqYD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = imread('gan_images/5000.png')\n",
        "plt.imshow(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enu9ilcMqg7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = imread('gan_images/20000.png')\n",
        "plt.imshow(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH4lkYufqrDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = imread('gan_images/29800.png')\n",
        "plt.imshow(a)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}